{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import marius as m\n",
    "from marius.tools.preprocess.dataset import LinkPredictionDataset\n",
    "from pathlib import Path\n",
    "from marius.tools.preprocess.converters.torch_converter import TorchEdgeListConverter\n",
    "from marius.tools.preprocess.datasets.fb15k_237 import FB15K237\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "batch_size = 1024\n",
    "total_epoch = 10\n",
    "new_ratio = .1\n",
    "lr = .1\n",
    "ewc_lambda = 1e7\n",
    "ewc_type = 'ewc' # none, l2, ewc\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "device = torch.device('cuda')\n",
    "new_type = 'random' # strategy_edge, strategy_node, random, whole\n",
    "num_neigh_layers = 2\n",
    "dataset = 'FB15K237' # FB15K237, YAGO-3SP, IMDB-30SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "dataset not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ylyang0/data/GitHub/test/test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250726f6a353576696131227d/home/ylyang0/data/GitHub/test/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m             rems\u001b[39m.\u001b[39mappend(rem)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250726f6a353576696131227d/home/ylyang0/data/GitHub/test/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2250726f6a353576696131227d/home/ylyang0/data/GitHub/test/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdataset not supported\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: dataset not supported"
     ]
    }
   ],
   "source": [
    "def read_edges_csv(filename):\n",
    "    edges = []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split(',')\n",
    "            edges.append((int(line[0]), int(line[1]), int(line[2])))\n",
    "    return edges\n",
    "\n",
    "def read_dif_csv(filename):\n",
    "    add, rem = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split(',')\n",
    "            if line[0] == 'add':\n",
    "                add.append((int(line[1]), int(line[2]), int(line[3])))\n",
    "            else:\n",
    "                rem.append((int(line[1]), int(line[2]), int(line[3])))\n",
    "    return add, rem\n",
    "\n",
    "if dataset == 'FB15K237':\n",
    "    edges = read_edges_csv('train_convert.csv')\n",
    "    test_edges = read_edges_csv('test_convert.csv')\n",
    "    valid_edges = read_edges_csv('valid_convert.csv')\n",
    "    import random\n",
    "    random.shuffle(edges)\n",
    "    split = [.0, .5, .6, .7, .75, .8, .85, .9, .95, 1.0]\n",
    "elif dataset == 'YAGO-3SP':\n",
    "    Yedges = []\n",
    "    Yvalidedges = []\n",
    "    Ytestedges = []\n",
    "    adds, rems = [], []\n",
    "    edges, valid_edges, test_edges = [], [], []\n",
    "    for i in range(3):\n",
    "        Yedges.append(read_edges_csv(f'YAGO-3SP/train{i}.csv'))\n",
    "        Yvalidedges.append(read_edges_csv(f'YAGO-3SP/valid{i}.csv'))\n",
    "        Ytestedges.append(read_edges_csv(f'YAGO-3SP/test{i}.csv'))\n",
    "        if i > 0:\n",
    "            add, rem = read_dif_csv(f'YAGO-3SP/dif{i}.csv')\n",
    "            adds.append(add)\n",
    "            rems.append(rem)\n",
    "elif dataset == 'IMDB-30SP':\n",
    "    Iedges = []\n",
    "    Ivalidedges = []\n",
    "    Itestedges = []\n",
    "    adds, rems = [], []\n",
    "    edges, valid_edges, test_edges = [], [], []\n",
    "    for i in range(30):\n",
    "        Iedges.append(read_edges_csv(f'IMDB-30SP/train{i}.csv'))\n",
    "        Ivalidedges.append(read_edges_csv(f'IMDB-30SP/valid{i}.csv'))\n",
    "        Itestedges.append(read_edges_csv(f'IMDB-30SP/test{i}.csv'))\n",
    "        if i > 0:\n",
    "            add, rem = read_dif_csv(f'IMDB-30SP/dif{i}.csv')\n",
    "            adds.append(add)\n",
    "            rems.append(rem)\n",
    "else:\n",
    "    raise Exception('dataset not supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27136 37\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'FB15K237':\n",
    "    nrelation = 237\n",
    "    nentity = 14541\n",
    "elif dataset == 'YAGO-3SP':\n",
    "    nrelation = 37\n",
    "    nentity = 27136\n",
    "elif dataset == 'IMDB-30SP':\n",
    "    nrelation = 14\n",
    "    nentity = 312588\n",
    "print(nentity, nrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(embedding_dim, num_nodes, num_relations, device, dtype):\n",
    "    # setup shallow embedding encoder\n",
    "    embedding_layer = m.nn.layers.EmbeddingLayer(dimension=embedding_dim, device=device)\n",
    "    encoder = m.encoders.GeneralEncoder(layers=[[embedding_layer]])\n",
    "\n",
    "    # initialize node embedding table\n",
    "    emb_table = embedding_layer.init_embeddings(num_nodes)\n",
    "\n",
    "    # initialize DistMult decoder\n",
    "    decoder = m.nn.decoders.edge.DistMult(num_relations=num_relations,\n",
    "                                          embedding_dim=embedding_dim,\n",
    "                                          use_inverse_relations=True,\n",
    "                                          device=device,\n",
    "                                          dtype=dtype,\n",
    "                                          mode=\"train\")\n",
    "\n",
    "    loss = m.nn.SoftmaxCrossEntropy(reduction=\"sum\")\n",
    "\n",
    "    # metrics to compute during evaluation\n",
    "    reporter = m.report.LinkPredictionReporter()\n",
    "    reporter.add_metric(m.report.MeanReciprocalRank())\n",
    "    reporter.add_metric(m.report.MeanRank())\n",
    "    reporter.add_metric(m.report.Hitsk(1))\n",
    "    reporter.add_metric(m.report.Hitsk(10))\n",
    "\n",
    "    # sparse_lr sets the learning rate for the embedding parameters\n",
    "    model = m.nn.Model(encoder, decoder, loss, reporter, sparse_lr=lr)\n",
    "\n",
    "    # set optimizer for dense model parameters. In this case this is the DistMult relation (edge-type) embeddings\n",
    "    model.optimizers = [m.nn.AdamOptimizer(model.named_parameters(), lr=lr)]\n",
    "\n",
    "    return model, emb_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, batch):\n",
    "    scores = model.forward_lp(batch, True)\n",
    "    loss1 = model.loss_function(scores[0], scores[1], True)\n",
    "    loss2 = model.loss_function(scores[2], scores[3], True)\n",
    "    return loss1 + loss2\n",
    "\n",
    "def train_epoch(model, embeddings, train_edges, first_train, ewc):\n",
    "    #print('start epoch...')\n",
    "    train_neg_sampler = m.data.samplers.CorruptNodeNegativeSampler(num_chunks=10, num_negatives=500, degree_fraction=0.0, filtered=True)\n",
    "    dataloader = m.data.DataLoader(edges=torch.tensor(train_edges,dtype=torch.int32, device=device),\n",
    "                                        node_embeddings=embeddings,\n",
    "                                        batch_size=batch_size,\n",
    "                                        neg_sampler=train_neg_sampler,\n",
    "                                        filter_edges=[torch.tensor(edges, dtype=torch.int32, device=device)],\n",
    "                                        learning_task=\"lp\",\n",
    "                                        train=True)\n",
    "    dataloader.initializeBatches()\n",
    "\n",
    "    counter = 0\n",
    "    loss_function = model.loss_function\n",
    "    while dataloader.hasNextBatch():\n",
    "\n",
    "        batch = dataloader.getBatch()\n",
    "        model.optimizers[0].clear_grad()\n",
    "        batch.node_embeddings.requires_grad_()\n",
    "        loss = get_loss(model, batch)\n",
    "        if not first_train and ewc_type != 'none':\n",
    "            loss2 = ewc.get_loss(model.parameters(), batch)\n",
    "            #print(loss, loss2)\n",
    "            loss = loss + loss2\n",
    "        loss.backward()\n",
    "        model.optimizers[0].step()\n",
    "        batch.accumulateGradients(lr)\n",
    "        dataloader.updateEmbeddings(batch)\n",
    "\n",
    "        counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, embeddings):\n",
    "    eval_neg_sampler = m.data.samplers.CorruptNodeNegativeSampler(filtered=True)\n",
    "    dataloader = m.data.DataLoader(edges=torch.tensor(test_edges, dtype=torch.int32, device=device),\n",
    "                                        node_embeddings=embeddings,\n",
    "                                        batch_size=batch_size,\n",
    "                                        neg_sampler=eval_neg_sampler,\n",
    "                                        learning_task=\"lp\",\n",
    "                                        filter_edges=[torch.tensor(edges, dtype=torch.int32, device=device), torch.tensor(valid_edges, dtype=torch.int32, device=device), torch.tensor(test_edges, dtype=torch.int32, device=device)], # used to filter out false negatives in evaluation\n",
    "                                        train=False)\n",
    "    # need to reset dataloader before state each epoch\n",
    "    dataloader.initializeBatches()\n",
    "\n",
    "    counter = 0\n",
    "    while dataloader.hasNextBatch():\n",
    "\n",
    "        batch = dataloader.getBatch()\n",
    "        model.evaluate_batch(batch)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "    model.reporter.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "class EWC:\n",
    "    def __init__(self, ewc_lambda):\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "    \n",
    "    def update_mean(self, paras, embeddings):\n",
    "        self.paras = []\n",
    "        for para in paras:\n",
    "            self.paras.append(para.data.clone())\n",
    "        self.embeddings = embeddings.data.clone()\n",
    "    \n",
    "    def update_fisher(self, paras, embeddings, loss):\n",
    "        self.paras_fishers = []\n",
    "        grads = autograd.grad(loss, [embeddings])\n",
    "        #for i in range(len(paras)):\n",
    "        #    self.paras_fishers.append(grads[i].data.clone() ** 2)\n",
    "        self.embeddings_fishers = grads[-1].data.clone() ** 2\n",
    "    \n",
    "    def update(self, paras, batch, loss, embeddings):\n",
    "        self.update_mean(paras, batch.node_embeddings)\n",
    "        self.update_fisher(paras, batch.node_embeddings, loss)\n",
    "        self.embeddings = embeddings.clone()\n",
    "        temp_embeddings = self.embeddings_fishers\n",
    "        self.embeddings_fishers = torch.zeros(embeddings.shape, device=device, requires_grad=False)\n",
    "        self.embeddings_fishers[batch.unique_node_indices] = temp_embeddings\n",
    "\n",
    "    def get_loss(self, paras, batch):\n",
    "        losses = []\n",
    "        nodes = batch.unique_node_indices\n",
    "        #for i in range(len(paras)):\n",
    "        #    para, saved_para, saved_fisher = paras[i], self.paras[i], self.paras_fishers[i]\n",
    "        #    if ewc_type == 'ewc':\n",
    "        #        losses.append((saved_fisher * (para - saved_para) ** 2).sum()/len(para))\n",
    "        #    else:\n",
    "        #        losses.append((ewc_lambda * (para - saved_para) ** 2).sum()/len(para))\n",
    "        if ewc_type == 'ewc':\n",
    "            losses.append((self.embeddings_fishers[nodes] * (batch.node_embeddings - self.embeddings[nodes]) ** 2).sum()/len(nodes))\n",
    "        else:\n",
    "            losses.append((ewc_lambda * (batch.node_embeddings - self.embeddings[nodes]) ** 2).sum()/len(nodes))\n",
    "        return self.ewc_lambda / 2 * sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(model, embeddings, edges):\n",
    "    eval_neg_sampler = m.data.samplers.CorruptNodeNegativeSampler(filtered=False)\n",
    "    dataloader = m.data.DataLoader(edges=torch.tensor(edges, dtype=torch.int32, device=device),\n",
    "                                        node_embeddings=embeddings,\n",
    "                                        batch_size=400000,\n",
    "                                        neg_sampler=eval_neg_sampler,\n",
    "                                        learning_task=\"lp\",\n",
    "                                        train=False)\n",
    "    dataloader.initializeBatches()\n",
    "    return dataloader.getBatch()\n",
    "\n",
    "def forward(model, embeddings, edges):\n",
    "    batch = get_batch(model, embeddings, edges)\n",
    "    assert batch is not None\n",
    "    values = model.forward_lp(batch, train=False)[0].cpu().detach().numpy()\n",
    "    return batch.edges.cpu().detach().numpy(), values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_part_edges(model, embeddings, edges, new_ratio):\n",
    "    if len(edges)==0:\n",
    "        return []\n",
    "    edges, values = forward(model, embeddings, edges)\n",
    "    values = values[:len(edges)]\n",
    "    new_size = int(len(edges)*new_ratio)\n",
    "    threshold = values[np.argpartition(values, - new_size)[-new_size]]\n",
    "    return list(edges[values > threshold])\n",
    "def get_lower_part_edges(model, embeddings, edges, new_ratio):\n",
    "    if len(edges)==0:\n",
    "        return []\n",
    "    edges, values = forward(model, embeddings, edges)\n",
    "    values = values[:len(edges)]\n",
    "    new_size = int(len(edges)*new_ratio)\n",
    "    threshold = values[np.argpartition(values, - new_size)[new_size]]\n",
    "    return list(edges[values < threshold])\n",
    "def get_part_edges(model, embeddings, edges, new_ratio):\n",
    "    return get_upper_part_edges(model, embeddings, edges, new_ratio/2) +\\\n",
    "            get_lower_part_edges(model, embeddings, edges, new_ratio/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affected_nodes(edges):\n",
    "    nodes = set()\n",
    "    for edge in edges:\n",
    "        nodes |= set([edge[0], edge[2]])\n",
    "    return nodes\n",
    "    \n",
    "def get_filtered_edges(edges, new_edges):\n",
    "    nodes = get_affected_nodes(new_edges)\n",
    "    ans = []\n",
    "    for edge in edges:\n",
    "        if edge[0] in nodes or edge[2] in nodes:\n",
    "            ans.append(edge)\n",
    "    return ans\n",
    "\n",
    "def get_dis_to_nodes(model, embeddings, new_edges):\n",
    "    dis = [0 for i in range(nentity+1)]\n",
    "    new_edges, values = forward(model, embeddings, new_edges)\n",
    "    for edge, value in zip(new_edges, values):\n",
    "        dis[edge[0]] += 1/value\n",
    "        dis[edge[2]] += 1/value\n",
    "    return dis\n",
    "\n",
    "def get_edges_from_dis(edges, dis):\n",
    "    new_size = int(len(dis)*new_ratio)\n",
    "    threshold = dis[np.argpartition(dis, - new_size)[-new_size]]\n",
    "    ans = []\n",
    "    for edge in edges:\n",
    "        if dis[edge[0]] > threshold or dis[edge[2]] > threshold:\n",
    "            ans.append(edge)\n",
    "    return ans\n",
    "\n",
    "def get_random_nodes():\n",
    "    import random\n",
    "    nodes = list(range(nentity+1))\n",
    "    random.shuffle(nodes)\n",
    "    return set(nodes[:int(len(nodes)*new_ratio)])\n",
    "\n",
    "def get_edges_from_nodes(edges, nodes):\n",
    "    ans = []\n",
    "    for edge in edges:\n",
    "        if edge[0] in nodes or edge[2] in nodes:\n",
    "            ans.append(edge)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(nodes):\n",
    "    for edge in edges:\n",
    "        if edge[0] in nodes or edge[2] in nodes:\n",
    "            nodes |= set([edge[0], edge[2]])\n",
    "    return nodes\n",
    "\n",
    "def get_affected_degree_of_nodes(model, embeddings, edges):\n",
    "    ans = [0 for i in range(nentity+1)]\n",
    "    new_edges, values = forward(model, embeddings, edges)\n",
    "    values = values - min(values) + 1\n",
    "    for edge, value in zip(new_edges, values):\n",
    "        ans[edge[0]] += 1/value\n",
    "        ans[edge[2]] += 1/value\n",
    "    return ans\n",
    "\n",
    "def get_weighted_edges(model, embeddings, new_edges, new_size, old_edges):\n",
    "    nodes = get_affected_nodes(new_edges)\n",
    "    for _ in range(num_neigh_layers):\n",
    "        nodes = get_neighbors(nodes)\n",
    "    degree = get_affected_degree_of_nodes(model, embeddings, new_edges)\n",
    "    total_degree = .0\n",
    "    for edge in old_edges:\n",
    "        if edge[0] in nodes or edge[2] in nodes:\n",
    "            total_degree += degree[edge[0]] + degree[edge[2]]\n",
    "    ans = []\n",
    "    for edge in old_edges:\n",
    "        if np.random.random() < new_size * (degree[edge[0]] + degree[edge[2]])/total_degree:\n",
    "            ans.append(edge)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, embeddings = init_model(embed_size, nentity, nrelation, device, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewc = EWC(ewc_lambda)\n",
    "if dataset == 'FB15k237':\n",
    "    for t in range(1, len(split)):\n",
    "        old_edges = edges[:int(len(edges)*split[t-1])]\n",
    "        new_edges = edges[int(len(edges)*split[t-1]):int(len(edges)*split[t])]\n",
    "\n",
    "        if t > 1:\n",
    "            batch = get_batch(model, embeddings, old_edges)\n",
    "            batch.node_embeddings.requires_grad_()\n",
    "            loss = get_loss(model, batch)\n",
    "            if t > 2:\n",
    "                assert (old_embeddings == ewc.embeddings).all()\n",
    "            ewc.update(model.parameters(), batch, loss, embeddings)\n",
    "            delta = 0\n",
    "            delta += ((embeddings - old_embeddings) ** 2).sum()\n",
    "            delta += ((old_paras[0] - model.parameters()[0]) ** 2).sum()\n",
    "            delta += ((old_paras[1] - model.parameters()[1]) ** 2).sum()\n",
    "            print(f\"delta: {delta}\")\n",
    "\n",
    "        old_embeddings = embeddings.clone()\n",
    "        old_paras = [x.clone() for x in model.parameters()]\n",
    "        if new_type == 'strategy_edge':\n",
    "            old_edges = get_lower_part_edges(model, embeddings, old_edges, new_ratio)\n",
    "            train_edges = old_edges + new_edges\n",
    "        elif new_type == 'strategy_node':\n",
    "            old_edges = get_filtered_edges(old_edges, new_edges)\n",
    "            #dis = get_dis_to_nodes(model, embeddings, new_edges)\n",
    "            #old_edges = get_edges_from_dis(old_edges, dis)\n",
    "            train_edges = old_edges + new_edges\n",
    "        elif new_type == 'random':\n",
    "            import random\n",
    "            random.shuffle(old_edges)\n",
    "            train_edges = old_edges[:int(len(old_edges)*new_ratio)]+new_edges\n",
    "        elif new_type == 'whole':\n",
    "            train_edges = old_edges + new_edges\n",
    "        else:\n",
    "            nodes = get_random_nodes()\n",
    "            old_edges = get_edges_from_nodes(old_edges, nodes)\n",
    "            train_edges = old_edges + new_edges\n",
    "        print(f't:{t}, training size: {len(train_edges)}, total size: {int(len(edges)*split[t])}')\n",
    "\n",
    "        for epoch in range(total_epoch):\n",
    "            train_epoch(model, embeddings, train_edges, t==1, ewc)\n",
    "            \n",
    "            if epoch == total_epoch-1:\n",
    "                eval_epoch(model, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:0, training size: 123974, total size: 123974\n",
      "[2022-08-20 13:47:59.698] [info] [reporting.cpp:106] \n",
      "=================================\n",
      "Link Prediction: 6000 edges evaluated\n",
      "MRR: 0.273109\n",
      "Mean Rank: 645.876667\n",
      "Hits@1: 0.177667\n",
      "Hits@10: 0.454333\n",
      "=================================\n",
      "delta: 364593.9375\n",
      "t:1, training size: 13282, total size: 124724\n",
      "[2022-08-20 13:48:02.250] [info] [reporting.cpp:106] \n",
      "=================================\n",
      "Link Prediction: 6000 edges evaluated\n",
      "MRR: 0.246015\n",
      "Mean Rank: 694.124333\n",
      "Hits@1: 0.149000\n",
      "Hits@10: 0.437333\n",
      "=================================\n",
      "delta: 9890.23046875\n",
      "t:2, training size: 13457, total size: 125574\n",
      "[2022-08-20 13:48:04.868] [info] [reporting.cpp:106] \n",
      "=================================\n",
      "Link Prediction: 6000 edges evaluated\n",
      "MRR: 0.252877\n",
      "Mean Rank: 736.309333\n",
      "Hits@1: 0.157333\n",
      "Hits@10: 0.440667\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'YAGO-3SP':\n",
    "    for t in range(3):\n",
    "        new_edges = []\n",
    "        if t == 0:\n",
    "            new_edges = Yedges[0]\n",
    "            old_edges = []\n",
    "            valid_edges = Yvalidedges[0]\n",
    "            test_edges = Ytestedges[0]\n",
    "            edges = new_edges\n",
    "        else:\n",
    "            new_edges = []\n",
    "            old_edges = []\n",
    "            old_edgeset = set(Yedges[t-1])\n",
    "            for edge in Yedges[t]:\n",
    "                if edge in old_edgeset:\n",
    "                    old_edges.append(edge)\n",
    "                else:\n",
    "                    new_edges.append(edge)\n",
    "            valid_edges = Yvalidedges[t]\n",
    "            test_edges = Ytestedges[t]\n",
    "            edges = new_edges + old_edges\n",
    "\n",
    "        if t >= 1:\n",
    "            batch = get_batch(model, embeddings, old_edges)\n",
    "            batch.node_embeddings.requires_grad_()\n",
    "            loss = get_loss(model, batch)\n",
    "            if t >= 2:\n",
    "                assert (old_embeddings == ewc.embeddings).all()\n",
    "            ewc.update(model.parameters(), batch, loss, embeddings)\n",
    "            for para in model.parameters():\n",
    "                para.requires_grad_(False)\n",
    "            delta = 0\n",
    "            delta += ((embeddings - old_embeddings) ** 2).sum()\n",
    "            delta += ((old_paras[0] - model.parameters()[0]) ** 2).sum()\n",
    "            delta += ((old_paras[1] - model.parameters()[1]) ** 2).sum()\n",
    "            print(f\"delta: {delta}\")\n",
    "\n",
    "        old_embeddings = embeddings.clone()\n",
    "        old_paras = [x.clone() for x in model.parameters()]\n",
    "        if new_type == 'strategy_edge':\n",
    "            old_edges = get_lower_part_edges(model, embeddings, old_edges, new_ratio)\n",
    "            train_edges = old_edges + new_edges\n",
    "        elif new_type == 'strategy_node':\n",
    "            new_size = int(len(old_edges)*new_ratio)\n",
    "            old_edges = get_weighted_edges(model, embeddings, new_edges, new_size, old_edges)\n",
    "            train_edges = old_edges + new_edges\n",
    "        elif new_type == 'random':\n",
    "            import random\n",
    "            random.shuffle(old_edges)\n",
    "            train_edges = old_edges[:int(len(old_edges)*new_ratio)]+new_edges\n",
    "        elif new_type == 'whole':\n",
    "            train_edges = old_edges + new_edges\n",
    "        else:\n",
    "            nodes = get_random_nodes()\n",
    "            old_edges = get_edges_from_nodes(old_edges, nodes)\n",
    "            train_edges = old_edges + new_edges\n",
    "        print(f't:{t}, training size: {len(train_edges)}, total size: {int(len(edges))}')\n",
    "\n",
    "        for epoch in range(total_epoch):\n",
    "            train_epoch(model, embeddings, train_edges, t==0, ewc)\n",
    "            \n",
    "            if epoch == total_epoch-1:\n",
    "                eval_epoch(model, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'IMDB-30SP':\n",
    "    for t in range(30):\n",
    "        new_edges = []\n",
    "        if t == 0:\n",
    "            new_edges = Iedges[0]\n",
    "            old_edges = []\n",
    "            valid_edges = Ivalidedges[0]\n",
    "            test_edges = Itestedges[0]\n",
    "            edges = new_edges\n",
    "        else:\n",
    "            new_edges = []\n",
    "            old_edges = []\n",
    "            old_edgeset = set(Iedges[t-1])\n",
    "            for edge in Iedges[t]:\n",
    "                if edge in old_edgeset:\n",
    "                    old_edges.append(edge)\n",
    "                else:\n",
    "                    new_edges.append(edge)\n",
    "            valid_edges = Ivalidedges[t]\n",
    "            test_edges = Itestedges[t]\n",
    "            edges = new_edges + old_edges\n",
    "\n",
    "        if t >= 1:\n",
    "            batch = get_batch(model, embeddings, old_edges)\n",
    "            batch.node_embeddings.requires_grad_()\n",
    "            loss = get_loss(model, batch)\n",
    "            if t >= 2:\n",
    "                assert (old_embeddings == ewc.embeddings).all()\n",
    "            ewc.update(model.parameters(), batch, loss, embeddings)\n",
    "            for para in model.parameters():\n",
    "                para.requires_grad_(False)\n",
    "            delta = 0\n",
    "            delta += ((embeddings - old_embeddings) ** 2).sum()\n",
    "            delta += ((old_paras[0] - model.parameters()[0]) ** 2).sum()\n",
    "            delta += ((old_paras[1] - model.parameters()[1]) ** 2).sum()\n",
    "            print(f\"delta: {delta}\")\n",
    "\n",
    "        old_embeddings = embeddings.clone()\n",
    "        old_paras = [x.clone() for x in model.parameters()]\n",
    "        if new_type == 'strategy_edge':\n",
    "            old_edges = get_lower_part_edges(model, embeddings, old_edges, new_ratio)\n",
    "            train_edges = old_edges + new_edges\n",
    "        elif new_type == 'strategy_node':\n",
    "            new_size = int(len(old_edges)*new_ratio)\n",
    "            old_edges = get_weighted_edges(model, embeddings, new_edges, new_size, old_edges)\n",
    "            train_edges = old_edges + new_edges\n",
    "        elif new_type == 'random':\n",
    "            import random\n",
    "            random.shuffle(old_edges)\n",
    "            train_edges = old_edges[:int(len(old_edges)*new_ratio)]+new_edges\n",
    "        elif new_type == 'whole':\n",
    "            train_edges = old_edges + new_edges\n",
    "        else:\n",
    "            nodes = get_random_nodes()\n",
    "            old_edges = get_edges_from_nodes(old_edges, nodes)\n",
    "            train_edges = old_edges + new_edges\n",
    "        print(f't:{t}, training size: {len(train_edges)}, total size: {int(len(edges))}')\n",
    "\n",
    "        for epoch in range(total_epoch):\n",
    "            train_epoch(model, embeddings, train_edges, t==0, ewc)\n",
    "            \n",
    "            if epoch == total_epoch-1:\n",
    "                eval_epoch(model, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "30321e586485ff286409f7cb79c071d9ae3f6d75bc7fc88e030372b2ae4093dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
